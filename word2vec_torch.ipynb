{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: str, 日本語文\n",
    "    :return tokenized: list of str, トークナイズされたリスト\n",
    "    \"\"\"\n",
    "    tagger = MeCab.Tagger()\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokenized = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            tokenized.append(node.surface)\n",
    "        node = node.next\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['古池', 'や', '蛙', '飛び込む', '水', 'の', '音']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('古池や蛙飛び込む水の音')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabクラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "MIN_COUNT = 1\n",
    "word2id = {PAD_TOKEN: PAD, UNK_TOKEN:UNK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"語彙を管理するクラス\"\"\"\n",
    "    def __init__(self, word2id={}):\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v:k for k,v in self.word2id.items()}\n",
    "    \n",
    "    def build_vocab(self, corpus, min_count=1):\n",
    "        \"\"\"テキストから語彙を構築するメソッド\n",
    "        :pram corpus: list of list of str, コーパスとなるテキスト\n",
    "        :pram min_count: int\n",
    "        \"\"\"\n",
    "        word_counter = {} # 単語の出現回数をカウント\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "            \n",
    "        # min_count以上出現する単語のみを語彙に追加\n",
    "        # 出現回数の多い順にIDを振る\n",
    "    \n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "        # 語彙に含まれる単語の出現回数\n",
    "        self.row_vocab = {w:word_counter[w] for w in self.word2id.keys() if w in word_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語リストをIDのリストに変換する関数\n",
    "def sentence_to_ids(vocab, sentence):\n",
    "    \"\"\"\n",
    "    :pram vocab: vocabオブジェクト\n",
    "    :pram sentence: list of str\n",
    "    :return ids : list of int\n",
    "    \"\"\"\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOWの実装をやってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocab import *\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの定義\n",
    "class DataLoader_CBOW(object):\n",
    "    \"\"\"CBOW用のデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size=50, window=2):\n",
    "        \"\"\"\n",
    "        :pram text: 教師データ list of lists of int\n",
    "        :pram batch_size\n",
    "        :pram window\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.w_pointer = 0  # 単語単位のポインタ\n",
    "        self.s_pointer = 0  # 文単位のポインタ\n",
    "        self.n_sent = len(text)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        :return batch_X: (batch_size, window*2)のテンソル\n",
    "        :return batch_Y: (batch_size, 1)のテンソル\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        \n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sent = self.text[self.s_pointer]\n",
    "            target = sent[self.w_pointer]\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            one_x = sent[start:self.w_pointer] + sent[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            one_x = pad_seq(one_x, self.window * 2)\n",
    "            \n",
    "            batch_X.append(one_x)\n",
    "            batch_Y.append(target)\n",
    "            \n",
    "            self.w_pointer += 1\n",
    "            if self.w_pointer >= len(sent):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "            \n",
    "                if self.s_pointer >= self.n_sent:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "                \n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader_CBOW([[1, 2, 3, 4], [5, 6, 7, 8], [9, 1, 2, 3], [4, 5, 6, 7]], batch_size=2, window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2, 0],\n",
      "        [1, 3]]), tensor([1, 2]))\n",
      "(tensor([[2, 4],\n",
      "        [3, 0]]), tensor([3, 4]))\n",
      "(tensor([[6, 0],\n",
      "        [5, 7]]), tensor([5, 6]))\n",
      "(tensor([[6, 8],\n",
      "        [7, 0]]), tensor([7, 8]))\n",
      "(tensor([[1, 0],\n",
      "        [9, 2]]), tensor([9, 1]))\n",
      "(tensor([[1, 3],\n",
      "        [2, 0]]), tensor([2, 3]))\n",
      "(tensor([[5, 0],\n",
      "        [4, 6]]), tensor([4, 5]))\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=0)  # Embedding層の定義\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)  # 全結合層（バイアスなし）\n",
    "        \n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :pram batch_X: Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :pram batch_Y: Tensor(dtype=torch.long), (batch_size, 1)\n",
    "        :return loss: CBOWのロス\n",
    "        \"\"\"\n",
    "        \n",
    "        emb_X = self.emb(batch_X) # (batch_size, window*2, embedding_size)\n",
    "        sum_X = torch.sum(emb_X, dim=1)  # (batch_size, embedding_size)\n",
    "        lin_X = self.linear(sum_X)  # (batch_size, vocab_size)\n",
    "        log_prob_X = F.log_softmax(lin_X, dim=-1) # (batch_size, vocab_size)\n",
    "        loss = F.nll_loss(log_prob_X, batch_Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/kokoro.txt', 'r') as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.strip() for sent in sentences]\n",
    "sentences = [tokenize(sent) for sent in sentences]\n",
    "vocab = Vocab(word2id)\n",
    "vocab.build_vocab(sentences, min_count=3, min_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence_to_ids(vocab, sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(vocab_size=len(vocab.word2id), embedding_size=128)\n",
    "optimizer_cbow = optim.Adam(cbow.parameters())\n",
    "dataloader_cbow = DataLoader_CBOW(sentences, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, input, optimizer=None, is_train=True):\n",
    "    \"\"\"lossを計算するための関数\n",
    "    \n",
    "    is_train=Trueならモデルをtrainモードに、\n",
    "    is_train=Falseならモデルをevaluationモードに設定します\n",
    "    \n",
    "    :param model: 学習させるモデル\n",
    "    :param input: モデルへの入力\n",
    "    :param optimizer: optimizer\n",
    "    :param is_train: bool, モデルtrainさせるか否か\n",
    "    \"\"\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    # lossを計算します。\n",
    "    loss = model(*input)\n",
    "\n",
    "    if is_train:\n",
    "        # .backward()を実行する前にmodelのparameterのgradientを全て0にセットします\n",
    "        optimizer.zero_grad()\n",
    "        # parameterのgradientを計算します。\n",
    "        loss.backward()\n",
    "        # parameterのgradientを用いてparameterを更新します。\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:9.7713\n",
      "batch:100, loss:3.5774\n",
      "batch:200, loss:5.3056\n",
      "batch:300, loss:4.4193\n",
      "batch:400, loss:3.7120\n",
      "batch:500, loss:4.2474\n",
      "batch:600, loss:4.7219\n",
      "batch:700, loss:3.9432\n",
      "batch:800, loss:2.1595\n",
      "batch:900, loss:3.8819\n",
      "batch:1000, loss:3.5987\n",
      "Elapsed time: 6.07 [sec]\n"
     ]
    }
   ],
   "source": [
    "start_at = time.time()\n",
    "n_batches = 1000\n",
    "\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_cbow):\n",
    "    loss = compute_loss(cbow, (batch_X, batch_Y), optimizer=optimizer_cbow, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "\n",
    "end_at = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow.emb.weight.data.cpu().numpy(), \"./models/cbow_embedding.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"./data/cbow_embedding.pth\")\n",
    "\n",
    "# 保存したパラメータの読み込み方\n",
    "e = torch.load(\"./data/cbow_embedding.pth\")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_similarity(embedding_path, word, n):\n",
    "    \"\"\"\n",
    "    与えられた単語に最も似ている単語とcos類似度を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "    embedding = torch.load(embedding_path)\n",
    "\n",
    "    # 単語ベクトルを全て単位ベクトルにする\n",
    "    norm = np.linalg.norm(embedding, ord=2, axis=1, keepdims=True)\n",
    "    norm = np.where(norm==0, 1, norm) # 0で割ることを避ける\n",
    "    embedding /= norm\n",
    "    e = embedding[vocab.word2id[word]]\n",
    "\n",
    "    # 単語ベクトル同士のcos類似度を計算する\n",
    "    cos_sim = np.dot(embedding, e.reshape(-1, 1)).reshape(-1,)\n",
    "    most_sim = np.argsort(cos_sim)[::-1][1:n+1] # 自分は除く\n",
    "    most_sim_words = [vocab.id2word[_id] for _id in most_sim]\n",
    "    top_cos_sim = cos_sim[most_sim]\n",
    "    out = \", \".join([w+\"({:.4f})\".format(v) for w, v in zip(most_sim_words, top_cos_sim)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'準備(0.2844), 二人(0.2808), 落ち付い(0.2740), 大きな(0.2513), 近づく(0.2476)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_word_similarity('./models/cbow_embedding.pth', '先生', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'です'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.id2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
